{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducció a la segmentació semàntica\n",
    "\n",
    "**Professor**: Ramon Mateo Navarro\n",
    "\n",
    "**Assignatura**: Models d'intel·ligència artificial\n",
    "\n",
    "**Link** : [Image segmentation with a U-Net-like architecture](https://keras.io/examples/vision/oxford_pets_image_segmentation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquest notebook explicarem pas a pas un exemple de segmentació semàntica. Per fer-ho seguirem el tutorial \"Image segmentation with a U-net-like architecture\" que es troba disponible en la pròpia pàgina de Keras. Aquest tutorial ens servirà com a punt d'entrada al món de la segmentació."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descarregant les dades\n",
    "\n",
    "El primer pas és descarregar el dataset. Això automàticament descarrega les dades i les descomprimeix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz\n",
    "!curl -O https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz\n",
    "\n",
    "!tar -xf images.tar.gz\n",
    "!tar -xf annotations.tar.gz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Tots els imports necessaris per la realització d'aquest notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "from keras.utils import load_img\n",
    "from PIL import ImageOps\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow import image as tf_image\n",
    "from tensorflow import io as tf_io\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparació del dataset\n",
    "\n",
    "Primer indicarem els paths on es troben les imatges i després on es troben les seves segmentacions. Un cop fet fent llistes per compressió obtindrem tots els paths de totes les imatges i les seves respectives màscares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dir = \"images/\"\n",
    "target_dir = \"annotations/trimaps/\"\n",
    "img_size = (160, 160)\n",
    "num_classes = 3\n",
    "batch_size = 32\n",
    "\n",
    "input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(input_dir, fname)\n",
    "        for fname in os.listdir(input_dir)\n",
    "        if fname.endswith(\".jpg\")\n",
    "    ]\n",
    ")\n",
    "target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(target_dir, fname)\n",
    "        for fname in os.listdir(target_dir)\n",
    "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Number of samples:\", len(input_img_paths))\n",
    "\n",
    "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
    "    print(input_path, \"|\", target_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATENCIÓ**, és molt important quan treballem amb aquest tipus de dades i tenim diferents conjunts assegurar-nos que es correspon la X amb la Y . Molts cops les imatges poden no està ben aparellada i fer-nos perdre molt de temps fins que ho detectem. Assegurar-vos sempre de printejar i veure sí l'input és correspont amb el output que desitgeu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualitzant un exemple\n",
    "\n",
    "Anem a veure que tenim dins del dataset ara. Primer printejem l'imatge original i posteriorment la seva respectiva màscara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display input image #7\n",
    "display(Image(filename=input_img_paths[9]))\n",
    "\n",
    "# Display auto-contrast version of corresponding target (per-pixel categories)\n",
    "img = ImageOps.autocontrast(load_img(target_img_paths[9]))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veiem tres colors no? Què està passant? Si analitzem la màscara veurem que tenim tres classes diferents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(load_img(target_img_paths[9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparació del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "    batch_size,\n",
    "    img_size,\n",
    "    input_img_paths,\n",
    "    target_img_paths,\n",
    "    max_dataset_len=None,\n",
    "):\n",
    "    \"\"\"Returns a TF Dataset.\"\"\"\n",
    "\n",
    "    def load_img_masks(input_img_path, target_img_path):\n",
    "        # carrega l'imatge d'entrada\n",
    "        input_img = tf_io.read_file(input_img_path)\n",
    "        input_img = tf_io.decode_png(input_img, channels=3)\n",
    "        input_img = tf_image.resize(input_img, img_size)\n",
    "        input_img = tf_image.convert_image_dtype(input_img, \"float32\")\n",
    "\n",
    "        #carrega la mascara, el nostre target\n",
    "        target_img = tf_io.read_file(target_img_path)\n",
    "        target_img = tf_io.decode_png(target_img, channels=1)\n",
    "        target_img = tf_image.resize(target_img, img_size, method=\"nearest\")\n",
    "        target_img = tf_image.convert_image_dtype(target_img, \"uint8\")\n",
    "\n",
    "        # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "        target_img -= 1\n",
    "        return input_img, target_img\n",
    "\n",
    "    # For faster debugging, limit the size of data\n",
    "    if max_dataset_len:\n",
    "        input_img_paths = input_img_paths[:max_dataset_len]\n",
    "        target_img_paths = target_img_paths[:max_dataset_len]\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((input_img_paths, target_img_paths))\n",
    "    dataset = dataset.map(load_img_masks, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definició del model\n",
    "\n",
    "Anem a definir ara el model. Recordem que volem implementar un model com U-net. Aquest model té moltes variants i diferents arquitectures. Podeu provar diferents models sobre el mateix dataset i veure quin us genera un millor resultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(img_size, num_classes):\n",
    "    inputs = keras.Input(shape=img_size + (3,))\n",
    "\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = get_model(img_size, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creació del dataset de Test i validació\n",
    "\n",
    "Ara toca separar el dataset en train i validació per podre entrenar el nostre model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_samples = 1000\n",
    "random.Random(1337).shuffle(input_img_paths)\n",
    "random.Random(1337).shuffle(target_img_paths)\n",
    "train_input_img_paths = input_img_paths[:-val_samples]\n",
    "train_target_img_paths = target_img_paths[:-val_samples]\n",
    "val_input_img_paths = input_img_paths[-val_samples:]\n",
    "val_target_img_paths = target_img_paths[-val_samples:]\n",
    "\n",
    "# Instantiate dataset for each split\n",
    "# Limit input files in `max_dataset_len` for faster epoch training time.\n",
    "# Remove the `max_dataset_len` arg when running with full dataset.\n",
    "train_dataset = get_dataset(\n",
    "    batch_size,\n",
    "    img_size,\n",
    "    train_input_img_paths,\n",
    "    train_target_img_paths,\n",
    "    max_dataset_len=1000,\n",
    ")\n",
    "valid_dataset = get_dataset(\n",
    "    batch_size, img_size, val_input_img_paths, val_target_img_paths\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenament del model\n",
    "\n",
    "Ara toca entrenar el model. Per aquesta primera iteració es seteja com a funció de pèrdua la Sparse Categorical Cross Entropy ja que recordem que no ho tenim setejat en one hot encoding. Com a optimitzador farem servir Adam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)  # Convert y_pred to float32\n",
    "    y_true = tf.squeeze(y_true, axis=-1)  # Remove the last dimension from y_true if it exists\n",
    "    y_true = tf.cast(y_true, tf.float32)  # Convert y_true to float32\n",
    "    intersection = tf.reduce_sum(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n",
    "    union = tf.reduce_sum(tf.cast(tf.greater(y_true + y_pred, 0), tf.float32))\n",
    "    smooth = tf.constant(1e-7)\n",
    "    return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)  # Obtén la clase predicha\n",
    "    y_pred = tf.cast(y_pred, tf.float32)  # Convierte a float32\n",
    "    y_true = tf.squeeze(y_true, axis=-1)  # Quita la última dimensión de y_true si existe\n",
    "    y_true = tf.cast(y_true, tf.float32)  # Convierte a float32\n",
    "\n",
    "    intersection = tf.reduce_sum(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n",
    "\n",
    "    smooth = tf.constant(1e-7)  # Factor para evitar división por cero\n",
    "    dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    return dice\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-4), loss=\"sparse_categorical_crossentropy\", metrics=[mean_iou, dice_coefficient]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\", save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 50\n",
    "mdl_hist = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualització de l'entrenament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(mdl_hist.history['loss'], label='Loss - Entrenamiento')\n",
    "plt.plot(mdl_hist.history['val_loss'], label='Loss - Validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Pérdida')\n",
    "plt.title('Pérdida durante el Entrenamiento')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(mdl_hist.history['mean_iou'], label='IOU - Entrenamiento')\n",
    "plt.plot(mdl_hist.history['val_mean_iou'], label='IOU - Validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('IOU')\n",
    "plt.title('Índice de Intersección sobre Unión durante el Entrenamiento')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(mdl_hist.history['dice_coefficient'], label='dice - Entrenamiento')\n",
    "plt.plot(mdl_hist.history['val_dice_coefficient'], label='dice - Validación')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Dice')\n",
    "plt.title('Dice coeffient durante el Entrenamiento')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualització de les mascaras\n",
    "\n",
    "Ara toca visualitzar que tan bé ho ha fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = get_dataset(\n",
    "    batch_size, img_size, val_input_img_paths, val_target_img_paths\n",
    ")\n",
    "val_preds = model.predict(val_dataset)\n",
    "\n",
    "\n",
    "def display_mask(i):\n",
    "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
    "    mask = np.argmax(val_preds[i], axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = ImageOps.autocontrast(keras.utils.array_to_img(mask))\n",
    "    display(img)\n",
    "\n",
    "\n",
    "# Display results for validation image #10\n",
    "i = 10\n",
    "\n",
    "# Display input image\n",
    "display(Image(filename=val_input_img_paths[i]))\n",
    "\n",
    "# Display ground-truth target mask\n",
    "img = ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n",
    "display(img)\n",
    "\n",
    "# Display mask predicted by our model\n",
    "display_mask(i)  # Note that the model only sees inputs at 150x150.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
