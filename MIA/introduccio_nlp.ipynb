{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduccio a a NLP amb Pytorch\n",
    "\n",
    "**Assignatura**: Models d'intel·ligència artificial\n",
    "\n",
    "**Professor** : Ramon Mateo Navarro\n",
    "\n",
    "En aquest notebook aprendrem algunes de les bases del NLP. En el següent encara profunditzarem més en aquesta tasca.\n",
    "\n",
    "Aquest tutorial està basat en el de Microsoft learns el qual s'ha adaptat per aquest curs i els vostres coneixements. Podeu trobar el tutorial en aquest [Link](https://learn.microsoft.com/es-es/training/modules/intro-natural-language-processing-pytorch/2-represent-text-as-tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requisits\n",
    "\n",
    "Primer de tot instal·larem les llibreries essencials per poder fer això. En el següent .txt s'han recopilat les més importants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchtext\n",
    "%pip install -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install portalocker>=2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquest notebook, començarem amb una senzilla tasca de classificació de text basada en el conjunt de dades de mostra ***AG.NEWS***, que consisteix a classificar els titulars de notícies en una de les 4 categories: .World, Sports, Business i Sci/Tech... Aquest conjunt de dades es construeix a partir del mòdul ``torchtext``. de ``PyTorch``, de manera que podem accedir-hi fàcilment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import os\n",
    "import collections\n",
    "torch.utils.data.datapipes.utils.common.DILL_AVAILABLE = torch.utils._import_utils.dill_available()\n",
    "import torchdata\n",
    "import torchtext.datasets\n",
    "os.makedirs('./data',exist_ok=True)\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podeu observar com hem creat un directori que es dirà data on guardarem allà el dataset. Després hem creat una llista on tindrem els .\n",
    "\n",
    "Aquí, trainsetdataset i test_dataset contenen iterators que retornen parells d'etiqueta (nombre de classe) i text respectivament, per exemple:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o iterant..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in zip(range(5),train_dataset):\n",
    "    print(f\"**{classes[x[0]]}** -> {x[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Així que com ara el volem fer servir diverses vegades per tal de fer-ho més fàcil ho transformarem en una llista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
    "train_dataset = list(train_dataset)\n",
    "test_dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenització i vectorització\n",
    "\n",
    "Ara necessitem convertir text en **números** que es puguin representar com a tensors per alimentar-los en una xarxa neuronal. El primer pas és convertir text a tokens - *tokenization**. Si utilitzem la representació a nivell de paraula, cada paraula estaria representada pel seu propi token. Utilitzarem el tokenizer integrat des del mòdul ``torchtext``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilitzarem el tokenizer de PyTorch per dividir paraules i espais en els primers 2 articles de notícies. En el nostre cas, utilitzem BASIC forenglish per al tokenizer per entendre l'estructura del llenguatge. Això retornarà una llista de cadenes del text i els caràcters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = train_dataset[0][1]\n",
    "second_sentence = train_dataset[1][1]\n",
    "\n",
    "f_tokens = tokenizer(first_sentence)\n",
    "s_tokens = tokenizer(second_sentence)\n",
    "\n",
    "print(f'\\nfirst token list:\\n{f_tokens}')\n",
    "print(f'\\nsecond token list:\\n{s_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "Abans de proseguir fem una petita pausa i anem a entendre un concepte que està molt relacionat aqui, els **word embeddings**. \n",
    "\n",
    "Els \"word embeddings\" o incrustacions de paraules són una tècnica utilitzada en el processament del llenguatge natural per representar les paraules en forma de vectors de números. La idea bàsica és transformar cada paraula en un vector dens que captura informació sobre el seu significat i el context en què sol aparèixer. Aquests vectors són creats de manera que les paraules que tenen significats similars o que s'utilitzen en contextos similars estiguin representades per vectors que estan propers entre si en l'espai vectorial.\n",
    "\n",
    "**Com funcionen?** \n",
    "\n",
    "Quan es crea un \"word embedding\", el model aprendrà a assignar vectors a les paraules de manera que la distància entre els vectors reflecteixi les relacions semàntiques i sintàctiques entre les paraules. Això s'aconsegueix mitjançant el entrenament en un gran corpus de text, on el model utilitza el context d'una paraula (les paraules que l'envolten) per predir la paraula mateixa.\n",
    "\n",
    "**Per què són útils?**\n",
    "* **Reducció de la Dimensionalitat**: Convertir paraules en vectors compactes redueix la complexitat i facilita el maneig computacional.\n",
    "* **Similitud Semàntica**: Permet als models de NLP detectar similituds semàntiques entre paraules, millorant així la qualitat de tasques com la cerca de text, la classificació de text i més.\n",
    "* **Flexibilitat**: Poden ser utilitzats en una varietat d'aplicacions de NLP, des de sistemes de resposta automàtica fins a anàlisi de sentiments.\n",
    "\n",
    "![](images_lab_nlp\\word_embeddings_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuació, per convertir text a números, necessitarem construir un vocabulari de tots els tokens. Primer construïm el diccionari utilitzant l'objecte ``Counter`` i després creem un objecte ``Vocab`` que ens ajudaria a fer front a la vectorització:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = torchtext.vocab.Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per veure com cada paraula s'assigna al vocabulari, farem un bucle a través de cada paraula de la llista per cercar el seu número d'índex en ``vocab``. Cada paraula o caràcter es mostra amb l'índex corresponent. Per exemple, la paraula ``the`` apareix diverses vegades en ambdues frases i és un índex únic en el vocab és el nombre **3**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lookup = [list((vocab[w], w)) for w in f_tokens]\n",
    "print(f'\\nIndex lockup in 1st sentence:\\n{word_lookup}')\n",
    "\n",
    "word_lookup = [list((vocab[w], w)) for w in s_tokens]\n",
    "print(f'\\nIndex lockup in 2nd sentence:\\n{word_lookup}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilitzant vocabulari, podem codificar fàcilment la nostra cadena tokenitzada en un conjunt de nombres. Fem servir el primer article de notícies com a exemple:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size if {vocab_size}\")\n",
    "\n",
    "def encode(x):\n",
    "    return [vocab.stoi[s] for s in tokenizer(x)]\n",
    "\n",
    "vec = encode(first_sentence)\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquest codi, el diccionari torchtext ``vocab.stoi``. ens permet convertir des d'una representació de cadena en números (el nom stoi* significa \"des de *s**tring *a** *i**ntegers). Per a tornar a convertir el text d'una representació numèrica en text, podem utilitzar el diccionari ``vocab.itos`` per a realitzar una cerca inversa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
