{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b53fb5-b2f5-49fc-9d75-c5ffb05863bc",
   "metadata": {},
   "source": [
    "# Como descargar Llama 2 y ejecutarlo en local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c7faf6-595b-4b7c-a8c8-6d52ee5a63b9",
   "metadata": {},
   "source": [
    "## Descargar Llama 2\n",
    "1. El primer paso será ir a la página oficial de Llama https://llama.meta.com . Alli rellenamos el formulario y recibiremos un correo con la URL para descargarlos. \n",
    "\n",
    "2. Una vez recibamos el correo deberemos ir a https://github.com/facebookresearch/llama y descargarnos el respositorio ya sea con git clone o dandole a descargar. \n",
    "\n",
    "3. Una vez tengamos descargado ejecutaremos el fichero download.sh . Allí se nos pedirá una url que será la que habremos recibido en el correo. Una vez introducida nos permitirá seleccionar que modelo queremos. Le damos y nos lo descargará todo.\n",
    "\n",
    "## Transformando llama a modelo GLM para su uso fácil. \n",
    "1. Instalaremos la libreria transformers haciendo !pip install transformers\n",
    "2. En la carpeta raiz de Llama veremos que tenemos un fichero llamado tokenizer.model , este lo copiaremos dentro de la carpeta del modelo que queramos usar. Si por ejemplo quiero usar llama2-7b-chat dentro deberemos tener los ficheros consolidated.pth , params.json y tokenizer.model\n",
    "3. Crearemos la carpeta llama-2-7b-chat-hf para saber que el modelo será el de hf y no tener confusión.\n",
    "4. Una vez hecho esto ejecutaremos la instrucción que está justo debajo.\n",
    "\n",
    "   \n",
    "***Nota***: En el model size cambiad el valor por el que vayais a usar, lo mismo que para el input_dir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "61cb1a3b-421c-4188-b33e-de47bea80c02",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Fetching all parameters from the checkpoint at llama-2-70b-chat/.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python3 -m transformers.models.llama.convert_llama_weights_to_hf  --model_size 70B  --input_dir llama-2-70b-chat/  --output_dir llama-2-70b-chat-hf/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb322c-2934-401b-9d74-da28b8cbee3d",
   "metadata": {},
   "source": [
    "## Transformando nuestro modelo a GGML\n",
    "\n",
    "1. Para poder usar el modelo aun mas facilmente lo que haremos será pasarlo a formato GGML. Para ello descargaremos llama cpp. Para hacerlo deberemos dirigirnos a https://github.com/ggerganov/llama.cpp y hacer un clone del respositorio o descargarlo. \n",
    "\n",
    "2. Una vez tengamos el respositorio deberemos instalar primero todos los requerimientos y dependecias que tiene. Para ello ejecutar la celda que está justo debajo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7917789-efec-4269-b54f-6f8d54925e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd llama.cpp |pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abedaef-11d2-4c02-ace0-fb8663a30561",
   "metadata": {},
   "source": [
    "Finalmente ejecutaremos la siguiente comanda que nos permitirá terminar de transformar nuestro modelo. Para ello deberemos ejecutar el script convert.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa419c43-40a7-40cb-b423-fd5c07e3c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 llama.cpp/convert.py  ../llama/llama-2-70b-chat-hf/  --outfile models/70B/ggml-model-f16.bin  --outtype f16  --vocab-dir ../llama/llama-2-70b-chat-hf/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88563ab7-3bad-4919-a14d-4c6ddc3006a9",
   "metadata": {},
   "source": [
    "## Chateando con Llama\n",
    "\n",
    "1. Importaremos Llama de llama_cpp\n",
    "2. Indicaremos el path donde se encuentra el binario de nuestro modelo.\n",
    "3. Declararemos el objeto LLM\n",
    "4. Finalmente para chatear solo hará falta escribir llm(\"lo que le queremos preguntar\") y esperar la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6216bb3c-24cf-4a72-b675-3df36bcc22d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 363 tensors from models/13B/ggml-model-f16.bin (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = llama\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type  f16:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 24.24 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = llama\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 24826.58 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   400.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    11.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    81.40 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '2048', 'general.name': 'llama', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '40', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'tokenizer.ggml.model': 'llama', 'general.file_type': '1'}\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "model_path = \"models/13B/ggml-model-f16.bin\"\n",
    "llm = Llama(model_path=model_path, n_ctx=4096)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b004cf6-7ae3-415b-ac78-bdd01acccf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4557.57 ms\n",
      "llama_print_timings:      sample time =     171.21 ms /   490 runs   (    0.35 ms per token,  2861.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  469531.21 ms /   490 runs   (  958.23 ms per token,     1.04 tokens per second)\n",
      "llama_print_timings:       total time =  470782.07 ms /   491 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-3f97eb45-cb79-415c-bf0c-51db53c3d23c',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1707228112,\n",
       " 'model': 'models/13B/ggml-model-f16.bin',\n",
       " 'choices': [{'text': '\\n====================================================================\\n\\nLos modelos grandes de lenguaje natural, como GPT (Generative Pre-trained Transformer), son una clase de algoritmos de aprendizaje profundo que han demostrado ser muy efectivos en una amplia variedad de tareas de procesamiento del lenguaje natural, como la generación de texto, la traducción automática, la respuesta a preguntas, la clasificación de texto y muchas otras. Estos modelos se han entrenado en grandes conjuntos de datos de lenguaje natural y han aprendido a extraer patrones y relaciones en el lenguaje que les permiten realizar estas tareas con gran precisión.\\n\\nEn términos generales, los modelos grandes de lenguaje natural funcionan como follows:\\n\\n1. Se entrena el modelo en un conjunto de datos de lenguaje natural, utilizando un algoritmo de aprendizaje profundo como el algoritmo de transformers.\\n2. Durante el entrenamiento, el modelo aprende a extraer patrones y relaciones en el lenguaje a partir del conjunto de datos de entrenamiento.\\n3. Después de entrenar el modelo, se puede utilizar para realizar una variedad de tareas de procesamiento del lenguaje natural, como generar texto, traducir texto a otro idioma, responder a preguntas, clasificar texto en diferentes categorías, etc.\\n\\nAlgunas formas en que se pueden utilizar los modelos grandes de lenguaje natural incluyen:\\n\\n1. Generación de texto: los modelos grandes de lenguaje natural pueden utilizarse para generar texto coherente y natural en una variedad de contextos, como la generación de reseñas de productos, la creación de contenido para sitios web, la generación de texto para chatbots y asistentes virtuales, etc.\\n2. Traducción automática: los modelos grandes de lenguaje natural pueden utilizarse para traducir texto de un idioma a otro. Esto se logra mediante el entrenamiento del modelo en un conjunto',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 22, 'completion_tokens': 490, 'total_tokens': 512}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Que son los modelos grandes de lenguaje natural como gpt y como los podemos usar?\",max_tokens=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75911695-53f0-4b14-a28d-4dd23f6fd26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "483e2a6b-3e76-48fb-a3e9-bd5fcf8969b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-42b2fe36-447c-48de-823e-0f9be62d1f5d',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1707224399,\n",
       " 'model': 'models/13B/ggml-model-f16.bin',\n",
       " 'choices': [{'text': '\\n\\nLos modelos grandes de lenguaje natural, como GPT (',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 22, 'completion_tokens': 16, 'total_tokens': 38}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab9108-96dc-419a-9c30-58f488704d63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
