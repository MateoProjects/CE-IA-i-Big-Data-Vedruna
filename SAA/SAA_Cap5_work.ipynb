{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducció a l'aprenentatge per reforç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignatura** : Sistemes d'aprenentatge automàtic\n",
    "\n",
    "**Professor** : Ramon Mateo Navarro\n",
    "\n",
    "**Credits i referències**: [https://towardsdatascience.com/q-learning-for-beginners-2837b777741]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llibreries necessàries\n",
    "\n",
    "Per aquesta pràctica necessitarem instal·lar **Gym**. **Gym** és un framework que conté entorns de múltiples jocs desenvolupat per OpenAI. És un framework pensat per poder entrenar models de RL. \n",
    "\n",
    "L'objectiu principal de **Gym** és oferir una interfície estàndard per a entorns de RL, permetent als investigadors i desenvolupadors entrenar els seus models de manera consistent i comparar els resultats de manera justa. Gym s'ocupa de la part de l'entorn del bucle d'aprenentatge per reforç, deixant que l'usuari es concentri en el disseny i l'implementació de l'algoritme d'aprenentatge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import display, clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquesta primera pràctica farem servir l'entorn frozen lake. Frozen lake és un joc en el que l'agent ha d'arribar a un destí sense caure pels forats.\n",
    "\n",
    "![](images_w5\\1_frozen_lake.gif)\n",
    "![](images_w5\\2_frozen_lake.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nostre objectiu serà programar i fer que l'agent aprengui a arribar a meta sense caure en el forat. \n",
    "\n",
    "El primer que farem serà crear l'entorn de joc. Gym disposa de múltiples jocs pel que el primer que haurem de fer es indicar quín volem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")\n",
    "env.reset(seed=42)\n",
    "result = env.render()\n",
    "plt.imshow(result[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hem observat ara com és el joc. Observarem ara com es pot obtenir l'espai d'accions possibles. Observarem com el nostre entorn pot executar fins a quatre accions diferents. Després també podem veure els possibles estats que pot tenir el joc, en aquest cas setze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creant la Q-table\n",
    "\n",
    "Ho tenim tot ja, accions i possibles estats. Per tant nem a crear ara la nostre Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_states = env.observation_space.n  # = 16\n",
    "nb_actions = env.action_space.n      # = 4\n",
    "qtable = np.zeros((nb_states, nb_actions))\n",
    "\n",
    "# Let's see how it looks\n",
    "print('Q-table')\n",
    "print(qtable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acabem d'inicialitzar la nostra Q-table amb tots els valors ara a zero.\n",
    "\n",
    "Repasem els comandaments: \n",
    "\n",
    "    - 0: LEFT\n",
    "    - 1: DOWN\n",
    "    - 2: RIGHT\n",
    "    - 3: UP\n",
    "\n",
    "Aquests valors han sigut obtinguts del propi codi: [Frozen Lake](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py#L10) \n",
    "\n",
    "Anem a veure ara això com funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(2)\n",
    "result = env.render()\n",
    "plt.imshow(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podem observar com el nostre amic s'ha mogut cap a la dreta.\n",
    "\n",
    "Ara anem a fer les coses bé. environment.step() retorna fins a quatre valors. Aquests són:\n",
    "* new_state: el nou estat del joc\n",
    "* reward: la recompensa per efectuar aquell moviment o pas\n",
    "* done: si el joc ha finalitzat o no\n",
    "* info: informació extra que en aquest cas no serà necessària.\n",
    "\n",
    "Anem ara a programar i entrenar el model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams.update({'font.size': 17})\n",
    "\n",
    "# inicialitzem la q-table\n",
    "qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "\n",
    "\n",
    "print('Q-table before training:')\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenant el model. Afegim que pugui renderitzar el joc. Aquests es visualitzarà cada 50 episodis per veure la seva evolució. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparàmetres\n",
    "episodes = 1000        # Número total d'episodis, seria com els epochs\n",
    "alpha = 0.8            # Learning rate\n",
    "gamma = 0.95           # Discount factor\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.005            # Exponential decay rate for exploration prob\n",
    "\n",
    "# Llista de resultats \n",
    "outcomes = []\n",
    "rewards = []\n",
    "# renderització\n",
    "render = False\n",
    "episodes_render = 50\n",
    "\n",
    "qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "outcomes = []\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    outcomes.append(\"Failure\")\n",
    "    while not done:\n",
    "          exp_exp_tradeoff = random.uniform(0, 1)\n",
    "\n",
    "          if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "\n",
    "          else:\n",
    "            action = env.action_space.sample()\n",
    "          \n",
    "          new_state, reward, done, info = env.step(action)\n",
    "\n",
    "          # actualitzem Q(s,a)\n",
    "          qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "\n",
    "          total_rewards += reward\n",
    "\n",
    "          # actualitzem l'estat del joc\n",
    "          state = new_state\n",
    "          if render and episodes % episodes_render == 0 :\n",
    "            result = env.render()\n",
    "            plt.figure(figsize=(2,2))\n",
    "            plt.imshow(result[0])\n",
    "            plt.axis('off')  # Ocultar ejes\n",
    "            display(plt.gcf())  # Mostrar figura actual\n",
    "            clear_output(wait=True)  # Limpiar la salida antes de la próxima visualización\n",
    "            \n",
    "            time.sleep(0.5)  # Esperar 0\n",
    "\n",
    "          # If we have a reward, it means that our outcome is a success\n",
    "          if reward:\n",
    "            outcomes[-1] = \"Success\"\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veient els resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "max_steps = 100\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        result = env.render()\n",
    "        plt.figure(figsize=(2,2))\n",
    "        plt.imshow(result[0])\n",
    "        plt.axis('off')  # \n",
    "        display(plt.gcf())  \n",
    "        clear_output(wait=True)  # \n",
    "        time.sleep(0.5)  # Esperar 0\n",
    "        \n",
    "        if done:\n",
    "            # Total de episodis necessaris.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
